{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitrisdais/generative-ai-lab/blob/summarization_task/notebooks/summarization_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9VV0exwnOp-"
      },
      "source": [
        "# 📝 Text Summarization with AI\n",
        "👋 Hi, I'm **Dimitris Dais**, an engineer passionate about AI creativity tools.\n",
        "\n",
        "This notebook demonstrates how to convert long documents into concise, informative summaries:  \n",
        "\n",
        "Step 1: Load and preprocess the text data  \n",
        "Step 2: BART as a Summarization Baseline  \n",
        "Step 3: Finetuned Models for Longer Contexts and Structured Abstraction  \n",
        "Step 4: Scaling Further: General-Purpose Open-Source LLMs (Qwen and Mistral)  \n",
        "\n",
        "For more explanation, refer to the [corresponding blog](https://dimitrisdais.github.io/dimitris-dais.github.io/nlp/llm/summarization_task/).\n",
        "\n",
        "Enjoyed it? Reuse or expand it — and feel free to connect.  \n",
        "\n",
        "🔗 **Website**: [dimitrisdais.github.io](https://dimitrisdais.github.io/dimitris-dais.github.io/)  \n",
        "📬 **Contact**: dimitris.dais.phd@gmail.com  \n",
        "🐙 **GitHub**: [@dimitrisdais](https://github.com/dimitrisdais)  \n",
        "🔗 **LinkedIn**: [linkedin.com/in/dimitris-dais](https://www.linkedin.com/in/dimitris-dais/)  \n",
        "▶️ **YouTube**: [youtube.com/@dimitrisdais](https://www.youtube.com/channel/UCuSdAarhISVQzV2GhxaErsg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![AI mastering the art of summarization.](https://raw.githubusercontent.com/dimitrisdais/generative-ai-lab/main/assets/images/robot_learning_to_summarize.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBAMjRa3cn7-"
      },
      "source": [
        "### 🔧 Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDT9vnywcnKC",
        "outputId": "41e7f749-e833-419d-a916-95b1aa575ae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting trafilatura\n",
            "  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2025.4.26)\n",
            "Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (3.4.2)\n",
            "Collecting courlan>=1.3.2 (from trafilatura)\n",
            "  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting htmldate>=1.9.2 (from trafilatura)\n",
            "  Downloading htmldate-1.9.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting justext>=3.0.1 (from trafilatura)\n",
            "  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (5.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2.4.0)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura)\n",
            "  Downloading tld-0.13.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting dateparser>=1.1.2 (from htmldate>=1.9.2->trafilatura)\n",
            "  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n",
            "Collecting lxml_html_clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura)\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n",
            "Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading courlan-1.3.2-py3-none-any.whl (33 kB)\n",
            "Downloading htmldate-1.9.3-py3-none-any.whl (31 kB)\n",
            "Downloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tld-0.13.1-py2.py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: tld, lxml_html_clean, dateparser, courlan, justext, htmldate, trafilatura\n",
            "Successfully installed courlan-1.3.2 dateparser-1.2.1 htmldate-1.9.3 justext-3.0.2 lxml_html_clean-0.4.2 tld-0.13.1 trafilatura-2.0.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install trafilatura\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwyvI3XiPvWA",
        "outputId": "57d5a7f2-4b0e-43b4-d7b7-dfeac158f2aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 PyTorch version: 2.6.0+cu124\n",
            "⚙️  CUDA available: True\n",
            "🖥️  CUDA device: Tesla T4\n",
            "🧱 Transformers version: 4.53.0.dev0\n",
            "💻 Python version: 3.11.12\n",
            "📦 BitsAndBytes version: 0.46.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import platform\n",
        "import transformers\n",
        "import bitsandbytes\n",
        "\n",
        "print(\"🧠 PyTorch version:\", torch.__version__)\n",
        "print(\"⚙️  CUDA available:\", torch.cuda.is_available())\n",
        "print(\"🖥️  CUDA device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
        "print(\"🧱 Transformers version:\", transformers.__version__)\n",
        "print(\"💻 Python version:\", platform.python_version())\n",
        "print(\"📦 BitsAndBytes version:\", bitsandbytes.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgIS5D6NR1Rw"
      },
      "source": [
        "### ⚙️ Check for GPU Availability\n",
        "\n",
        "This step checks whether a GPU is available in the current Colab environment and assigns the appropriate device (`\"cuda\"` for GPU or `\"cpu\"` otherwise).  \n",
        "Using a GPU can significantly speed up model inference for image and audio generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TldQjV5JQCJB",
        "outputId": "10358a19-07b8-436d-d3d6-ed0cf1ec6f1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTpa7BF6nOqE"
      },
      "source": [
        "## Step 1: 📁 Load and preprocess the text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T2cD_YqnOqE",
        "outputId": "f735212c-24b9-4599-d163-85bb083e99db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preview of extracted text (first 1000 chars):\n",
            "\n",
            "About Me\n",
            "📌 Open to new opportunities\n",
            "Hi, I am Dimitris Dais — a Senior Machine Learning Engineer with a PhD in Artificial Intelligence and Civil Engineering\n",
            "🔹 Experienced ML engineer delivering end-to-end AI solutions for complex, real-world challenges\n",
            "🔹 Skilled in defining problem scope, data strategy, and selecting optimal AI stacks\n",
            "🔹 Proven track record in AI innovation in academia (PhD, 10+ papers, 450+ citations) and industry\n",
            "🔹 Strong hands-on experience with multimodal & generative AI: VLMs/LLMs 🤗, transformers, prompt engineering, zero/few-shot learning\n",
            "🔹 Built and deployed cloud-based AI pipelines for real-time visual understanding and automated incident verification\n",
            "💼 Professional Experience\n",
            "Freelance — Remote\n",
            "Senior Machine Learning Engineer (Sep 2024 – Present)\n",
            "Athens, Greece / London, UK\n",
            "Working on real-time detection models and decision-support systems for defense and civil protection applications\n",
            "- Enhanced detection performance across diverse real-world scenarios by deve\n"
          ]
        }
      ],
      "source": [
        "import trafilatura\n",
        "\n",
        "url = \"https://dimitrisdais.github.io/dimitris-dais.github.io/\"\n",
        "downloaded = trafilatura.fetch_url(url)\n",
        "text = trafilatura.extract(downloaded)\n",
        "\n",
        "print(\"Preview of extracted text (first 1000 chars):\\n\")\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r56XxEkfnOqG"
      },
      "source": [
        "**📃 Text Cleaning and Preprocessing**  \n",
        "Before summarizing, it's important to clean and normalize the extracted text. This helps the language model better understand the structure and content, especially when dealing with long and semi-structured inputs like CVs. We remove unnecessary characters, collapse whitespace, and ensure paragraphs are clearly separated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynFaxafunOqH",
        "outputId": "67a3142c-c006-4719-930d-d21bc19cca5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Preview of cleaned text (first 1000 chars):\n",
            "\n",
            "About Me\n",
            "\n",
            " Open to new opportunities\n",
            "\n",
            "Hi, I am Dimitris Dais a Senior Machine Learning Engineer with a PhD in Artificial Intelligence and Civil Engineering\n",
            "\n",
            " Experienced ML engineer delivering end-to-end AI solutions for complex, real-world challenges\n",
            "\n",
            " Skilled in defining problem scope, data strategy, and selecting optimal AI stacks\n",
            "\n",
            " Proven track record in AI innovation in academia (PhD, 10+ papers, 450+ citations) and industry\n",
            "\n",
            " Strong hands-on experience with multimodal & generative AI: VLMs/LLMs , transformers, prompt engineering, zero/few-shot learning\n",
            "\n",
            " Built and deployed cloud-based AI pipelines for real-time visual understanding and automated incident verification\n",
            "\n",
            " Professional Experience\n",
            "\n",
            "Freelance Remote\n",
            "\n",
            "Senior Machine Learning Engineer (Sep 2024 Present)\n",
            "\n",
            "Athens, Greece / London, UK\n",
            "\n",
            "Working on real-time detection models and decision-support systems for defense and civil protection applications\n",
            "\n",
            "- Enhanced detection performance across diverse real-world scenarios by devel\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Clean and normalize raw extracted HTML text for better LLM input.\n",
        "\n",
        "    - Removes extra spaces and tabs\n",
        "    - Collapses multiple line breaks\n",
        "    - Strips non-ASCII characters (e.g., emojis)\n",
        "    - Adds double line breaks between sections using heuristics (e.g., headings)\n",
        "\n",
        "    Returns: Cleaned text string\n",
        "    \"\"\"\n",
        "    # Remove emojis and non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)          # Multiple spaces/tabs to single space\n",
        "    text = re.sub(r'\\n{2,}', '\\n', text)         # Collapse multiple line breaks\n",
        "    text = re.sub(r'\\n', '\\n\\n', text)           # Add double line breaks for structure\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Apply cleaning\n",
        "text = preprocess_text(text)\n",
        "\n",
        "# Preview\n",
        "print(\"🔍 Preview of cleaned text (first 1000 chars):\\n\")\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5PKdR69nOqH"
      },
      "source": [
        "## Step 2: 📉 BART as a Summarization Baseline  \n",
        "BART is a sequence-to-sequence transformer model pre-trained as a denoising autoencoder **and** fine-tuned for summarization tasks (e.g., CNN/DailyMail dataset). When you use it with the \"summarization\" pipeline in Hugging Face Transformers:\n",
        "\n",
        "✅ It automatically formats the input and output as a summarization task.\n",
        "\n",
        "✅ It tokenizes, truncates, and feeds the input to the model under the hood.\n",
        "\n",
        "⚠️ It doesn't craft custom prompts unless you do prompt-style input manually (for causal LMs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "6e5c170b372a4dfb88944658dcd93884",
            "76ce85dc523140c49f9029384422f4c6",
            "476fb76e5cb34fefb661a99a68af6158",
            "daab245ad2fe4ea6a1ed89ad3f1c4f7e",
            "cef6eac7598043f08aff10c76dcd0a66",
            "ab11ffe6a5464561a0caf62027c03482",
            "129d340704a54f03a3eaea4c02b4d076",
            "82c1dcb6f58c44d385f8c9968b6678c0",
            "c5a308ea51eb49c290db10462d2f6214",
            "8d87583fb7c14738a3d51765ac4f8715",
            "989f2f801ae84a85922a5b979f0f3d83",
            "1694a45fd3a84902911de040dff01134",
            "8b96d20ce93a4089ac5d48f38f1af9ee",
            "b08a8501486b4bceb6c3f83bc5e934dd",
            "4a17d28eebb5450a8c2a2f185f1f5b7b",
            "681e31a202834b278735832b9e556833",
            "00615fe3b5ea41a9b2e82366092d4c81",
            "fc033bc36ba44e1e83d77489ca0060a8",
            "a6d0146ce95744d296586f4dd7143a06",
            "3173dea20fb442b1bd78eb5a36dd6f5b",
            "6edc7eb6bfb443adba30d4b5e912fef3",
            "4064caaaeb2a45589aea9603f33452e9",
            "09fea052fe2e49c28fa4910ba2c38ef5",
            "503e3cadcf22427b8bd19c4166c019f6",
            "cc03be3b151244a7b5a9125dc224934c",
            "6d269b4583174dc89c9191e8b20bfb78",
            "d921827d4ba942c69421bbbb61b2eaf7",
            "0c07713c971f4ac69a09697ba2d4a2b2",
            "da6418c617034e47ba9af98b9c821707",
            "bb20794774fb45389d52c04dcdc048df",
            "be9f17b66e9a4c088cb1173e3167ed8f",
            "2d5e897235b142659cdd61d4fb770895",
            "d48a659172c542d0b85de8b4d2e09909",
            "2307ade3e54247f9b351099ee10ed2e3",
            "3de08f7efd004a4da876f38655037a64",
            "96c1de6bdbae43bab017b8651f9a6ab6",
            "b999b3b8ac93464f9dd60109619e078d",
            "92fadc9b77944a77a6fb508fdf904abf",
            "c2405242c1d3498ca5bebd83bbb46c3e",
            "ff0a18787a684d38a3bca007948b1528",
            "449cd66f9258423985c4b0d07b9f5c95",
            "2058c9a330d340cf8114cb58aa91cfe7",
            "e8d80af1bde24455bd6a5185ca06564a",
            "f5442f4aad154f0fa17a8e186d161898",
            "e433907a122948e09a97d4d93fecc45c",
            "f4a344af417a45508a58a338a4e68150",
            "97b5bd24d78b49cb914511bfa06ce284",
            "71216c3f216d428cb0b7ae1dcde3924c",
            "b8b06a103ba446b0a056b18774e7f30d",
            "12790dd65f0e418aa471804f17a3b61e",
            "5f16c6e6d33e4784bbb4ab12ea5792fa",
            "5461a6ade8a2439b9be6c8e4e55a48f5",
            "5edb0244b7984bb1b994166c205130c2",
            "14ecd6c67ec445058d77cc8845a93e4a",
            "8bd609a0dda6417bbf7a5f40a1e223d4",
            "a34f141480db457fb14371aa5dee1e1f",
            "b66337f8377a43969097aad843003b14",
            "311fb6a0bd2e4924b6c1c6813ee2256c",
            "619d45d90c8b4c15b7b8874589541d4c",
            "6d33c5f79ddb478e8d6ac2e482117fca",
            "6fd0a481469343648aafc525b9f10f4c",
            "bbd52ef81d7343ada826dd89e2c366a3",
            "f4add92c08ba45cba5002ca54bd63f9e",
            "e6cc2022d5ce4f06af1bf73beac3afd7",
            "931d37b7853141d9a6ff3f17b35fea21",
            "8ca4540c1ecc43efa459fd72dce65c7b"
          ]
        },
        "id": "MlB5MgrKnOqI",
        "outputId": "fde6159e-1cd8-4e6b-c99a-9758c15c6d35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e5c170b372a4dfb88944658dcd93884",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1694a45fd3a84902911de040dff01134",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09fea052fe2e49c28fa4910ba2c38ef5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2307ade3e54247f9b351099ee10ed2e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e433907a122948e09a97d4d93fecc45c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a34f141480db457fb14371aa5dee1e1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Define the Model & Pipeline\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "bart_model_name = \"facebook/bart-large-cnn\"\n",
        "\n",
        "tokenizer_bart = AutoTokenizer.from_pretrained(bart_model_name)\n",
        "\n",
        "summarizer_bart = pipeline(\n",
        "    \"summarization\",\n",
        "    model=bart_model_name,\n",
        "    device=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByKd1l9Evz5W"
      },
      "source": [
        "**Token-aware Chunking Function**  \n",
        "Since BART has a maximum input length of 1024 tokens, we want to chunk the text by token count, not character count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_1LOtvqv9UV"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, tokenizer, max_tokens=900):\n",
        "    print(\"🔧 Splitting text into chunks based on token count...\")\n",
        "    tokens = tokenizer.encode(text, truncation=False)\n",
        "    total_tokens = len(tokens)\n",
        "    print(f\"🔢 Total tokens in text: {total_tokens}\")\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0, total_tokens, max_tokens):\n",
        "        chunk_ids = tokens[i:i + max_tokens]\n",
        "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
        "        chunks.append(chunk_text)\n",
        "        print(f\"✅ Created chunk {len(chunks)} with {len(chunk_ids)} tokens\")\n",
        "\n",
        "    print(f\"📦 Total chunks: {len(chunks)}\")\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7StIFq0bwBZZ"
      },
      "source": [
        "Summarize Each Chunk (approx. 2 sentences)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rOtQcniwHY1"
      },
      "outputs": [],
      "source": [
        "def summarize_chunks(chunks):\n",
        "    summaries = []\n",
        "\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        print(f\"\\n🔍 Summarizing chunk {idx + 1}/{len(chunks)}...\")\n",
        "\n",
        "        summary = summarizer_bart(\n",
        "            chunk,\n",
        "            max_length=130,\n",
        "            min_length=80,\n",
        "            do_sample=False\n",
        "        )[0][\"summary_text\"]\n",
        "        summaries.append(summary)\n",
        "\n",
        "    return summaries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qinIwo-pwU02"
      },
      "source": [
        "Put It All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVWwr5b6wWH1",
        "outputId": "15fa0db4-6ed9-400a-d6b3-17bf9f2a3e9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Splitting text into chunks based on token count...\n",
            "🔢 Total tokens in text: 1126\n",
            "✅ Created chunk 1 with 900 tokens\n",
            "✅ Created chunk 2 with 226 tokens\n",
            "📦 Total chunks: 2\n",
            "\n",
            "🔍 Summarizing chunk 1/2...\n",
            "\n",
            "🔍 Summarizing chunk 2/2...\n"
          ]
        }
      ],
      "source": [
        "# 1. Chunk the long text\n",
        "chunks = chunk_text(text, tokenizer=tokenizer_bart)\n",
        "\n",
        "# 2. Summarize each chunk\n",
        "summaries = summarize_chunks(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx5EgjAw4_SC"
      },
      "source": [
        "⚠️ Note: Some sentences are off-topic due to mixed input. For better summaries, split the text into clear sections (e.g., profile, skills, experience) before chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugt7HWjb3zFN",
        "outputId": "8dfe90bb-c23a-40da-d24d-db50cdaf326c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📄 Full Summary:\n",
            "\n",
            "- Dimitris Dais is a Senior Machine Learning Engineer with a PhD in Artificial Intelligence and Civil Engineering.\n",
            "- He has built and deployed cloud-based AI pipelines for real-time visual understanding and automated incident verification using Vision-Language Models (VLMs) Dimitris has led the R&D of cutting-edge solutions for automatic industrial inspections and seismic damage assessment.\n",
            "- He also led a project applying AI to detect and monitor cracks on buildings under earthquake loads.\n",
            "- Programming & ML Frameworks: Python, PyTorch, TensorFlow, Keras, scikit-learn, ultralytics, Hugging Face, OpenAI.\n",
            "- Vision-Language Models (VLMs), transformers, LLM APIs, prompt engineering, zero/few-shot learning.\n",
            "- Retrieval-Augmented Generation (RAG) & Q&A: FAISS, SentenceTransformers, LlamaIndex, LangChain.\n",
            "- 3D Reconstruction: OpenCV, COLMAP, Open3D, Metashape.\n"
          ]
        }
      ],
      "source": [
        "# 3. Print results: One sentence per line\n",
        "print(\"📄 Full Summary:\\n\")\n",
        "for summary in summaries:\n",
        "    sentences = summary.split('. ')\n",
        "    for s in sentences:\n",
        "        s = s.strip()\n",
        "        if s:\n",
        "            print(f\"- {s.rstrip('.')}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tno8LfB8eO8v"
      },
      "source": [
        "## Step 3: 🔍 Finetuned Models for Longer Contexts and Structured Abstraction  \n",
        "While BART provides a strong starting point for summarization, its limited input size and occasionally shallow abstraction make it less suitable for complex or lengthy documents. In contrast, recent models like **PEGASUS-X** extend the transformer architecture to accommodate significantly longer contexts—allowing them to better preserve structure and meaning across larger spans of text.\n",
        "\n",
        "Two such models evaluated here are:\n",
        "\n",
        "- [`pszemraj/pegasus-x-large-book_synthsumm-bf16`](https://huggingface.co/pszemraj/pegasus-x-large-book_synthsumm-bf16)\n",
        "- [`BEE-spoke-data/pegasus-x-base-synthsumm_open-16k`](https://huggingface.co/BEE-spoke-data/pegasus-x-base-synthsumm_open-16k)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i7q_tLPetPE"
      },
      "source": [
        "## **`pszemraj/pegasus-x-large-book_synthsumm-bf16`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "9f72abe4a2c14010abfbde50626fae1b",
            "afae8cc9b1b74d36b9dbf2958b2adb99",
            "b7e4b7e6e21241019487465a171eb781",
            "8d4c7a1795bc4212bab7cff9d9d0b8c2",
            "ed4e0c29587b451fae75f42eb783d6ef",
            "4df91cddad71464fbf66f9e9bcbd4cfa",
            "c2262b7e71054484bedaa12d897fec59",
            "c854c32571fe4c32bad0cb0bbcfd174b",
            "5d8cfd41b6a74fb7987f2568318a6f28",
            "bd7a05564ea0451db9d16504074930bb",
            "9ff1ba8bac954672af153297ac210492",
            "767bab7b23a54fa7b27ec5afdd8ca8cb",
            "ff4682420bec452b9b64bea131bf32f0",
            "321f3459d31c49b199bec697b15c47b4",
            "d2e91053f69b4ee8945d9223bf33070f",
            "0d6aa5b877994862ba171cd885b6d854",
            "56fb97465dbb462d93f5b00bfa4736cd",
            "07de2a8b125d4b6e9c0e81e0fa0febe7",
            "222779b19f5d4311899b27717df5b868",
            "85f0dea1ae8d4ffe95d3d25bbaf3ecc5",
            "22f5a3b7869f4f0ebbb9ae6a5ec4560a",
            "0a471b437847442d8069a729908c1cd1",
            "8d85f86c398e4cfda780fc3a941da137",
            "46b58732d0124cf1883896ab9e254c37",
            "368988ff984440378d7675318d4ac6b0",
            "53787d4c7cc1425c9b04fa9b5593985e",
            "03037df63b7145a29098b8dda1537304",
            "60e7dc96a3014ad98eecd7f19e0de10a",
            "6f2e0607fb1d4e1ea3a8b493cd88e5a9",
            "356b58b6f42e427380733f27106872aa",
            "32b8c74bb4074b03b940360280df7854",
            "db56427cceb243969675d9f8320b7b4b",
            "d834d1468ebc48909dd8907cacae6ed7",
            "492ead648db945f4ac09e386670fafee",
            "503757806b8b42a28f5dd0b642aca1ce",
            "cdf4c1bb8d2645859a3ae1a080b6034f",
            "9c3de135d6254f89a9605fb3e94b629e",
            "d5a0141492d94d5ebd9003f5efca3f75",
            "03fa247c78ea4519a803a0a3ff3d6b6f",
            "5fcc2fb7d0fa40fa865d77c8e04ba6f1",
            "0877607faeb042c48da8942db61ad056",
            "8a1e2c518734469b9830cbf1e638b63a",
            "7bdc291542c242d59866f90d32c67659",
            "7b62f5476c034c0caadc091402805e04",
            "f7f907ad6fcc40898cebcbe51f0192ed",
            "b579f191eae44051aaa02a4462c8d2da",
            "e55f9b4c0acd48688982ba0392639711",
            "6b05b4b137bd485d9090c982a7497ac2",
            "73f7385e3fdf43d88c37178a535b1a35",
            "7f1f6d68ae5b41ceb5486203600a1b9e",
            "0e821117577842c08df53a94b7509a90",
            "1e34cf813c6c488cae3fdbea6754554f",
            "47171026fcdd4f2e80236d5e631baca7",
            "5de8e0914d6646718955d802019c1317",
            "0de83040f24c40869406cd2b36092dd7",
            "4bc7486e74b34a71b0afbcf6a60d8f1f",
            "85a771f74f194395a9a8cf8c420ff338",
            "5e846c94dd3c41848025a4f315b824a5",
            "22875a80e3d94a1c9f2bed6f97761dc1",
            "ec9524076cde426cbc3c31b95eb1276c",
            "49953e443be14fa583e8f584a998cf56",
            "87296ade6cc84a0c8610109411389a30",
            "f5909f6ec9534a3aa8cf19fd17b9ee50",
            "c2ba8237f93b46f18377fe7c2f9ef9c1",
            "269545fc2ff3454f803c6fface0a01e1",
            "69a117a3909c472e924c53a48e96cecf",
            "01e17a405ba145e9884b04299b05e0a0",
            "47e2834d780b476e9bfc05d675fa6319",
            "44ed6eb8b46347dd833b6950d645b732",
            "0c21e907ff4142da946bb8ce5a0ddd6f",
            "64047ee8a9114ccc9b6f75f6620a08e1",
            "4dea65653207481aa6397a784865efc8",
            "e36457ba09364b50a42d1a28e47a2dd8",
            "390b34ddd4a54fe495091aacfc5b35b0",
            "6188397510cd48a38065b0da2c2eca64",
            "a3f7f9877c834306b458c47391c5ea72",
            "1c20416e209d412383e3bdbbb6685e75"
          ]
        },
        "id": "gdtH1FWp9vGJ",
        "outputId": "e0ceb720-3980-49d9-ec4d-936814f27646"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f72abe4a2c14010abfbde50626fae1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "767bab7b23a54fa7b27ec5afdd8ca8cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d85f86c398e4cfda780fc3a941da137",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/336 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "492ead648db945f4ac09e386670fafee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/20.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7f907ad6fcc40898cebcbe51f0192ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bc7486e74b34a71b0afbcf6a60d8f1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/6.60M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01e17a405ba145e9884b04299b05e0a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "model_name = \"pszemraj/pegasus-x-large-book_synthsumm-bf16\"\n",
        "\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=model_name,\n",
        "    device=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI29RsgFEkjw",
        "outputId": "4eb3494f-5ad5-4177-b62b-e6469648f59d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📄 Summary:\n",
            "\n",
            "- Dimitris is a senior machine learning engineer with a PhD and extensive experience in AI and civil engineering.\n",
            "- He has developed and deployed AI solutions for defense, civil protection, industrial inspections, earthquake engineering, and 3D reconstruction.\n",
            "- He is actively seeking new opportunities and has experience in multimodal and generative AI, VLMs, and LLMs.\n",
            "- His professional profile includes experience in remote and freelance remote machine learning engineers, leading AI projects, and skills in programming and ML frameworks.\n"
          ]
        }
      ],
      "source": [
        "summary = summarizer(\n",
        "    \"Summarize the following professional profile in 2–3 sentences:\\n\\n\" + text,\n",
        "    max_length=300,\n",
        "    min_length=80,\n",
        "    do_sample=False\n",
        ")[0][\"summary_text\"]\n",
        "\n",
        "# 📄 Print each sentence on a new line\n",
        "print(\"📄 Summary:\\n\")\n",
        "for sentence in summary.split('. '):\n",
        "    s = sentence.strip()\n",
        "    if s:\n",
        "        print(f\"- {s.rstrip('.')}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjXCzj0seyHp"
      },
      "source": [
        "## **`BEE-spoke-data/pegasus-x-base-synthsumm_open-16k`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "41246b15cb584098a22d5e9f4d70840a",
            "9e16b19fba594289b09e7b53f1c29c2b",
            "33241161178543328ec51908a0d7c94f",
            "bfab4b6955e04539b3fcfb305696e87e",
            "53eb0b2065ab43f9865ab30b2340cb82",
            "ebf46e9834ca447ab6c8a160f3045ee5",
            "fd5bd7b54d974cacaefa960916143a92",
            "5e55073fdc1440449d44c932ff0dbe9a",
            "83227d2e6a8543a3ad75942be86f44a4",
            "b75ab1c367c143a4b7577620584de74f",
            "8ba67b00f14545c8bf5ce84653bdb27b",
            "a72d0362e52e4c82b6922b53b86939c2",
            "0fb43d6b46a54d608dfda8396c95830f",
            "74d957fc937c4d72b24e46ec916d2789",
            "55d61e381d334d3699baf74db7579851",
            "394c2c1a1b8343b08d1315053f18114d",
            "215f44ef8791447785c82cd4120ee455",
            "ecf1361868c4429b8dc7673d5cf7727e",
            "8b2b83931bb94f0b9aa129ae1f1377d4",
            "e49b0125f17946bc8b475bba20b32660",
            "7858278d758d45c78f8e92d8c6125b8a",
            "a397aa7f89cb4e8f92d031186499da2a",
            "17305c9a58264d0484f01db74fa35871",
            "4cd2616e1be74e3eaceb0a6e7d942907",
            "19399c1400ae4009988408119e97eb79",
            "a8cb25b7d11a4164bcfdcd138cb35a0b",
            "6333becc5f6a4fd3ae1781d22a894642",
            "ff85601fbb3a47f1a654b8f845d22fb7",
            "d9f84164cc804719bf048baeaf76134c",
            "14700a9dda75481591a6e71f43abc9f3",
            "b8b5e15cab7f4de8bb1d31f1d1872d6b",
            "409803541e944a0890ce08ed65b00a9b",
            "5b935ec293fc4e448399260a094d07bd",
            "e4cd23fd0afa48f7a4317c025bb72334",
            "869baa21bf4047319fc90616c3164b1b",
            "ee56100276d648e0a6831ff196bae6c3",
            "e3a22e5beb314de7b43657c1680d32a7",
            "cc8a65e094e54e78aace81f2c861321a",
            "e7ad86607afc4f7d8d73f747123854fd",
            "9e50146c94a34457bce82d8d90a58c1a",
            "525f881293a54ea585d802a9d5fc868f",
            "3f84a47c94a64f34aff46512311646b4",
            "8fd026e4bb0749f9936a556d514f9121",
            "0bbb36b298714bc18dd335f5aad91d3f",
            "e669ad5114ae4189b564a1f396076686",
            "2c8c968fa1bc456ebb016f0d74cd26bf",
            "77dc91c0878347a0b690e9db41ea530f",
            "31785cee6d4f47e0a6e0e4e11bbeece0",
            "a36293d3010f4043a529df0855132156",
            "d90165f58fb44ba78f4cee4a1a2798db",
            "643b959ae2134d56b64da42d690384d8",
            "c87db0533f9e4c33b61913e029b96318",
            "43bf4cbdb6d94cfbace227af30635ec9",
            "ee339379bf524621a59e2630d0a46116",
            "9358b89a578c4c58b25877ba4a21b5c4",
            "f822e1b81c8d45f0b596c05a4dd33844",
            "95883df1d51145dfb92281fd9eb0c6bc",
            "34bec43ef87d45a6b1ed7387bbfa6523",
            "165d1c0aef144805afcfddf6ccda4582",
            "c33c18b86adc474c8e062c1dc5f9c5a0",
            "d2d5d2e3cc364847b6a7360653f26ae0",
            "d697c9a4a7ae4e85838f930eb20a099a",
            "9471b18b324e4b3995394bcfeac0d72b",
            "accc83a286f94829b2fe3be1c0297610",
            "8d408b4b088c4d39ad36bc0ce1b87b78",
            "6a5e5177e8e04c2bbf096a9b600b020e",
            "e73f1983ae734259a459d2f70456c7b7",
            "d59ae436c90e42dd8698eea1ccd572fa",
            "f58342d5e5ce419b8bec10427d251047",
            "71d049c448a64b2b914c2c9cdccbe3cb",
            "aa5b2c99dd5f446c976908a0cafabade",
            "1194ba42b48a4449a9593d28e58ea41c",
            "f629dc042b5242f1ae8edbfdc91c0e7f",
            "89f01ec8d53040ba857849574cb96a2a",
            "7c1a72d3ebd14ea88c287c771fd8f820",
            "013c5abe38f34f25ad2ec1018d3e45c2",
            "a83d999222044e95b483c8173cd15f28"
          ]
        },
        "id": "JQscySXOFKbe",
        "outputId": "45d1aa3e-4262-4a83-da9c-6034004f3ce9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41246b15cb584098a22d5e9f4d70840a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a72d0362e52e4c82b6922b53b86939c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17305c9a58264d0484f01db74fa35871",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4cd23fd0afa48f7a4317c025bb72334",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/20.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e669ad5114ae4189b564a1f396076686",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f822e1b81c8d45f0b596c05a4dd33844",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/6.60M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e73f1983ae734259a459d2f70456c7b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_name = \"BEE-spoke-data/pegasus-x-base-synthsumm_open-16k\"\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=model_name,\n",
        "    device=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Npht0VhcFKbf",
        "outputId": "9c87118d-8141-4e81-fe43-c77f12a6c81f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📄 Summary:\n",
            "\n",
            "- Dimitris Dais, a Senior Machine Learning Engineer with a PhD in Artificial Intelligence and Civil Engineering, has worked on real-time detection models and decision-support systems for defense and civil protection applications in Athens, Greece, London, UK, Zurich, Switzerland, and Rotterdam, The Netherlands.\n",
            "- He has a strong background in multimodal & generative AI, including VLMs/LLMs, transformers, prompt engineering, zero/few-shot learning, and cloud-based AI pipelines for real-time visual understanding and automated incident verification.\n"
          ]
        }
      ],
      "source": [
        "summary = summarizer(\n",
        "    \"Summarize the following professional profile in 2–3 sentences:\\n\\n\" + text,\n",
        "    max_length=300,\n",
        "    min_length=80,\n",
        "    do_sample=False\n",
        ")[0][\"summary_text\"]\n",
        "\n",
        "# 📄 Print each sentence on a new line\n",
        "print(\"📄 Summary:\\n\")\n",
        "for sentence in summary.split('. '):\n",
        "    s = sentence.strip()\n",
        "    if s:\n",
        "        print(f\"- {s.rstrip('.')}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGZ0FcPzfFSU"
      },
      "source": [
        "## Step 4: 🧠 Scaling Further: General-Purpose Open-Source LLMs (Qwen and Mistral)  \n",
        "Beyond summarization-specific models, general-purpose large language models (LLMs) like **Qwen** and **Mistral** offer powerful summarization capabilities as a byproduct of their broader instruction-tuned design. These models are open-source, self-hostable, and often optimized for versatility across a wide range of tasks—including summarization, classification, reasoning, and dialogue.\n",
        "\n",
        "Models Evaluated:\n",
        "\n",
        "- [`Qwen/Qwen3-8B`](https://huggingface.co/Qwen/Qwen3-8B): a 8B-parameter instruction-tuned model from Alibaba, supporting multi-turn tasks and strong summarization performance.\n",
        "- [`mistralai/Mistral-7B-Instruct-v0.3`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3): a compact, high-performance 7B model optimized for following instructions and multi-task use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "4e404ad3ba904c47884e571f6601c05d",
            "31ffabda4c0749dd8b6f7f9e961a6931",
            "2d47d9fca88740edb946b5f95b907b0d",
            "7c6173b628d44d19b9250c0d5c18ff6c",
            "2b9ef9060de6472e892b4ccdb19b3129",
            "0acf8fdaf1aa4f2eb8b8593a99da035d",
            "dfabb3f6803c48eb9b03fb10f1669a26",
            "ff21d2d37ecc4f5b83ad46fc71c3e3d5",
            "9eedde0374124b1d8c51f6da968ddbfa",
            "a4f54f326dc54af6be087df56a38b0f3",
            "556bdaa067a0494293432f9f0d59e520",
            "9d83d02dec3f4984971e48cb0432b4bc",
            "aae7a01dde3e49f2bc270232409616e1",
            "d92074d4d5d34896ace4ff47e0c4880b",
            "ce371013c86d4cf09903abd058e74dc6",
            "cd6842bf34794a1e8871ee51beee8d87",
            "f2fcb7f89e154b4ca4f08fd71e1c276c",
            "d20025aad0674647b2c5ba112c9de0df",
            "fbc8ca5050604460936f5f076554ccb4",
            "1f24e87200b94d5fb0743da342fca16b"
          ]
        },
        "id": "yB67SrwSQpYw",
        "outputId": "f08636b3-2737-4da0-b12b-0153fa9e1962"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e404ad3ba904c47884e571f6601c05d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHxPzGOdPo_t"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTAQSb2SfnmU"
      },
      "source": [
        "## **`Qwen/Qwen3-8B`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693,
          "referenced_widgets": [
            "c076b7dbad14474e9f3fd9a1f74851d8",
            "a5b70293474d4470a03b47d3ee74bfa1",
            "147fa0b1948e4bebad2ac97e90aced43",
            "118b8b2a7e1c42f6b33f82fb1d665af1",
            "3c828828af804f128b7378cdfe24a60e",
            "df5fadf87b784de58b2796b06d406780",
            "ef185f92dbda43a08e879053f0e10d23",
            "fbb0079d05b7453a9f081f9cea1558fd",
            "6025c407e21f42ebade6da1a5d0c535b",
            "b64ec539507942bd904263490f2a6941",
            "975d2edc2b49442aaee3270169d59200",
            "9cb71fb09f574de5a21dda1e0b4d600e",
            "0fc0d43034474d44bfdd9b8ad47c72c9",
            "f3fc8d2b59684f46aeed5d58e926bff2",
            "860c663894a94a488c668e9a2827f6a7",
            "e9eb6de60e4a40d5a56022c92315f3b8",
            "629b26c883074685b8b812df3ca7955e",
            "1beba01001c140568003675b9282595e",
            "b69c551b615a45c689b9b9d20e64a7ce",
            "00b7e15a63fd4982aea4e9f70232235f",
            "c7b2635e4bf643d8a69ba3ac130e7013",
            "dedc9dfb803749ac8e242c1f31e8a483",
            "752b670c26074c85aa860e923287da92",
            "ac5f18c8ed8b40f0838c6f531b5e9eea",
            "c94ae968d683477fbdef6f275a7e99ab",
            "18f523b48deb4e1f9a64ea823727bfe4",
            "d34995300b754e16968f7ab6dfab3576",
            "f66c4496eeed4b49bd680a55c0deb828",
            "e9c9126a48854941803044915c120d67",
            "13cb7868bfd945fe8eac2a2cc51ab5e1",
            "8a55b7a49be244128993ef506cdb16f0",
            "a97f08d8bad74d1782043efbeabab5c2",
            "e2c615b3faee4383a67b2e9d7c73cd5f",
            "9f4c19fb2490424085bb532ce1ee31b4",
            "dcfafd9077e54698beda81d7b7ae87d4",
            "53c83685dadf4e1d9f302b6fd5a9f080",
            "60ce6b00d99648e2aa66530f69233400",
            "19871d8708794691a5e4fc30b65cf897",
            "7ad5dd7f1fcc4c678df573b4df9d9699",
            "8fbd008ea9f1440eb05d502df31252a5",
            "41493fe160904e7fafef7c479a393459",
            "b53460fb556541afb1af6df6e3b5c93d",
            "9074d567715e4ddd92461fdc51a0e09c",
            "7b18b37dd11342ab84b35b5f3d667f89",
            "48fe4629cfed4eff88a07f48b5417b01",
            "bc96ecde1b69466da715a8d1ecfd3b2a",
            "47b0ad1adc71441b90a20b07645b787c",
            "97379375fec64fec901dce5d8e9140d4",
            "47e68396288d48b3a3e46073aded6edb",
            "bcddc8883a8a47279fb9980c4443f57a",
            "20ead8db6bd9425083aac345c285bfc6",
            "2380b74df262424c90f76f40f01f27be",
            "bddd99afadca4163a4916ea0be977939",
            "e02bd02d15604901bc3a0543d2e1e597",
            "91e15d6499724382adfc9997887e1cc8",
            "0332c3f7397e4c24907110f8513bfadb",
            "f2842875993c43418fc1a5db885f7473",
            "4465826a3d82480eb6367894dbee1bd1",
            "3ef60317d09349238b76605656f4567c",
            "a2c97467ab0544cd83b72d5a1b1c7dca",
            "5a2961958dd6484db8fd7c9e5dfe9e76",
            "bbf63de637bb43debd5fce56041a872e",
            "85cf913631a541edb87574c55843b981",
            "dd839081d1bc4d7a935a3a3d52b7b01d",
            "314096d2a67a443fb71c4a93fb3c4289",
            "43262338b3b642b9b7f46df3aae946a1",
            "e22d670b81c24d4687cae5a3b51fe5be",
            "aed19d8f3e1f4e069351267529ca7f42",
            "bca1cfb3e36b41eeae4893433af689e4",
            "ee393fd023df410c9af394709c511f31",
            "1267398dd1e44aac95e5b3042bc1e41f",
            "befa614ac3354ce3a7cc0be9b651a412",
            "442c9dd7546e40b4992d9e0a733a47dd",
            "b1a8ce79f0be45929c2c9327fe532a74",
            "4b218e354c7249b8891f7d1f73b286c2",
            "34357c318c96487a826937b0bf3af226",
            "c75090f3e8a14bafa1e1a49783303076",
            "5edccd6686634d2bac45f8a0d783bdf7",
            "2ae90164434845e7bc9cd038b27f33b5",
            "cf690a37a5eb4fed9a8d2b9ea19bcd50",
            "287d2a6a86504e7a846a12b5dd890b01",
            "8e5ebc52898c4d6f8390d362ab2860e5",
            "43a09efdaa3a49c293856551d2b61f50",
            "3b0b09db9e6f4115a889946b0e88ba35",
            "1432a1232ece45c8aefee813fd07b083",
            "b30179ad11784c51b10e0a1c70669805",
            "b9f651737d024a3e83dbf7bdba9e9ce9",
            "ea4322a8a03548e19f47ea732c99f4b9",
            "79e8c0a0b1b94571b3c7f1bca38bc61a",
            "50fecd6621504c659358058437bddd2c",
            "dfef2ca4cbea4df6a1b5ba68e618ff0a",
            "a7b3c04c213f463db5af9daf659f6c11",
            "d9d1b62f8e954777be85ceb596678e2c",
            "bc6c0828ba2b49ae8542c77408eb6a30",
            "0a011f0889b84621bfa7346a5df058f1",
            "226953e4cba14fc2b539ec7d44d54429",
            "49030af9d6e742629fb8f40db3fb1acb",
            "26997845fd7c4f19bb708069c1bfd7af",
            "22abd239859741d0a0edb449f456ab5f",
            "5dd6c29be43642ecb97478e247bd96a4",
            "7b4cdd4deb0d48a5bf12ad9a7abcfe85",
            "ae21674de8104fceb821b0494a7f0687",
            "e2d69f46dc0a418aa0c2d096b13445cf",
            "fae7448e8e9242ea8402074ae8c707b2",
            "05305c5783914ba993ebfb2c4017875a",
            "e2260006eff947f48bfa11f395d16d1f",
            "0574b96438be4734b2947460bbadae91",
            "0df3a3cdb6834354ab666f74de9a2f12",
            "7cf7bf1e033c4d639b38b4e8084b3724",
            "69d22150b10544d58642d5a5165fbc56",
            "10831d9345484679b3164c6472a27074",
            "4a05c1c83b1d413bbd07a1198bf0cf4a",
            "22a8443863764ccebad82da0984011ea",
            "6ef953a6b991496e98477a3da653061d",
            "b190c6c4e5e3407e97920832be7c6979",
            "bcee030cdf92464f87073984f8cc28e2",
            "40cddb49729248dda37808c19a8f7256",
            "afa72f296a804b59a8beb58ab1a3e642",
            "27e521a17f934ea081858df735bc4613",
            "174e192229e348feb1c712a0a477de98",
            "90b140497e844da199d147b2ff711fb4",
            "bc2c49267982457183dd2038086c4aeb",
            "8ccefd52bd7440ed8eda0358fed101f6",
            "c6117f77dc4443558e74a93e4c2b2b23",
            "46cdd8d34d1b4900b7e2705596c49e7d",
            "9548458ee05e43079b6e7b2881f6395d",
            "b629e74abd3343c4ad631001b042e64d",
            "c2cc441dc1b345df8ad7ad6d6973cb26",
            "d5d1986d83b241d8a813955fed2c3f7c",
            "5ff71812d4054fb6ac6b35d187cd94ed",
            "8553036f7f17430e904616f4963446b9",
            "856661051aef42e393f89ced3e64064a",
            "6e6e62fc506e4bb3bdd0991511f68608",
            "bd19fcfb05ba420188fe271c4c27c24b",
            "8dab69be23c347b6885250750093aa23",
            "bdbb0a50573a42d5b0cd1c825553c37f",
            "3d1eada69a8f40fe8e4127381e77092d",
            "b6156f13a61547839bbeb420ba9c5d4b",
            "80c524f64bb44d4fae0dce135158fc66",
            "882bca3a25a94dd4b2aa5b48d3175fb7",
            "eac0c3b9537842ac8d9788797bbac010",
            "60ee87104edc4438916d6ea2da7ae409",
            "ba2cdf70ca284d3ea09bf092f9c1bfa9",
            "90f64589322f43619a46d3d4f999a314",
            "6714e20174d547e28d2c1f04df55e1d2",
            "e81488a3ddf14a16b86e0504a1b650bf",
            "21d995139dde4ad0a062660860eb986c",
            "15f1b874aebc4d458c649f464b03c133",
            "dcc7019d748240c3af6837168df15bfb",
            "9469071714fe4e40bc9dcbe9a0e6c043",
            "ed104afa707c44c2955405a875cfa291",
            "1fb4a994a89a438fa126ac15a017aca5",
            "9292b69671de44f68801404e064d5dd0",
            "83d6986f43f64bfe95b432ac8fd0247b"
          ]
        },
        "id": "wlv1XNIGIrbw",
        "outputId": "6499de50-85a8-4127-a271-a0abddac4118"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c076b7dbad14474e9f3fd9a1f74851d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/9.73k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cb71fb09f574de5a21dda1e0b4d600e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "752b670c26074c85aa860e923287da92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f4c19fb2490424085bb532ce1ee31b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48fe4629cfed4eff88a07f48b5417b01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0332c3f7397e4c24907110f8513bfadb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/32.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e22d670b81c24d4687cae5a3b51fe5be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5edccd6686634d2bac45f8a0d783bdf7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79e8c0a0b1b94571b3c7f1bca38bc61a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5dd6c29be43642ecb97478e247bd96a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10831d9345484679b3164c6472a27074",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc2c49267982457183dd2038086c4aeb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e6e62fc506e4bb3bdd0991511f68608",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90f64589322f43619a46d3d4f999a314",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_id = \"Qwen/Qwen3-8B\"\n",
        "\n",
        "# NEW: Proper quantization config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "# Load model with updated quantization config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG5v-lJkIrey"
      },
      "outputs": [],
      "source": [
        "def build_prompt(text):\n",
        "    return (\n",
        "        \"Please read the following professional profile and produce a concise 3-sentence summary. \"\n",
        "        \"The output should be formal, non-repetitive, and avoid chatbot commentary. \"\n",
        "        \"Do not include conversational elements or offer suggestions. Only write the summary:\\n\\n\"\n",
        "        f\"{text}\\n\\nSummary:\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmDWjpWRRsBP",
        "outputId": "c5b46bcf-5565-468c-e0df-9dd7090fd644"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📄 Qwen3-8B Summary:\n",
            "\n",
            "- Dimitris Dais is a Senior Machine Learning Engineer with expertise in developing end-to-end AI solutions for complex, real-world applications, particularly in defense, civil protection, and industrial inspection domains.\n",
            "- He has a strong academic background in Artificial Intelligence and Civil Engineering, with a PhD and a proven track record in AI innovation, including over 10 papers and 450+ citations.\n",
            "- His skills span multimodal and generative AI, cloud deployment, and cross-functional project leadership, supported by hands-on experience in both academic research and industry settings.\n",
            "- 2024.03.13.\n",
            "- 07:45:48.000 UTC+0.\n",
            "- 2024.03.13.\n",
            "- 07:45:48.000 UTC+0.\n",
            "- 2024.03.13.\n",
            "- 07:45:48.000 UTC+0.\n",
            "- 2024.03.13.\n",
            "- 07:45:48.000 UTC+0.\n",
            "- 2024.03.13.\n",
            "- 07:45:48.000 UTC+0.\n",
            "- 2024.03.13.\n",
            "- 07:45:48.000 UTC+0.\n",
            "- 2024.\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=300\n",
        ")\n",
        "\n",
        "prompt = build_prompt(text)\n",
        "response = pipe(prompt)[0][\"generated_text\"]\n",
        "\n",
        "# Extract the summary part only\n",
        "summary = response.split(\"Summary:\")[-1].strip()\n",
        "print(\"📄 Qwen3-8B Summary:\\n\")\n",
        "for s in summary.split('. '):\n",
        "    s = s.strip()\n",
        "    if s:\n",
        "        print(f\"- {s.rstrip('.')}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U-MvdX6f1nN"
      },
      "source": [
        "## **`mistralai/Mistral-7B-Instruct-v0.3`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401,
          "referenced_widgets": [
            "14a934778e8b4f5eb992657312585654",
            "17a5cec0f2684bf2b1c27fe0a43d0164",
            "8084a0c726214adf8d0e5a4b1a67fb7e",
            "740f89d5cdc34b54bef2d09af2415a87",
            "22b37202b92647528b63b99071eff100",
            "734f69ff40e3405d8982eabf40097f55",
            "727dc1e3e13e4785821daa4631ebb436",
            "a5d2c034b18d449f84f2a1735bed9a0b",
            "9b860cca6da64d078bb9152a1c4d7c00",
            "b827590895754f699cdb7a3dc53795a1",
            "f0f3145c5de7442081b9ee21f8bf36ee",
            "efa4db0b40014cc0b7b9da3fa77134e0",
            "e9dec6dddcb24b49892f53b7fe2f5dcb",
            "a6e2b90c992643f8921cfb4b383b1e8f",
            "2b1502dd01814d3cb4de28fcb1776056",
            "42eb814f6aa04252bd10804843bb8726",
            "1a1a8faff2c8488580b30e28286188c5",
            "3cde165e2c5a4ae6b8485e7b082dc56a",
            "59c9d542453d4876877e924919f8fe3d",
            "714c726ff77a4b43b67e31f58fb111bb",
            "989aaaa37f044143bc57d78d280429f4",
            "7868729585f9419895076cd6795af753",
            "cfbe05f155164ade847b667590706b70",
            "3df63cf9a3e649518bfcbee16d926334",
            "ef18df6f1ef84a2fa4afc2ae8b4a093d",
            "a24a247ae4cd4ea78e6c6630fafddb82",
            "a0ae13f9b82147b891f3a069e7f430cb",
            "d76b81a9127b49ec8cac73264e6fa00d",
            "1066188cc3a845f292a0a77ded21f6a6",
            "ef1afb792b874abe842aa1ea5dd6ddf4",
            "fb2dccc6e5a9451b90e9757b5edc84ac",
            "561186193b6c4a98aec61b5c14deba0c",
            "b42825d87a8d4db78250a37d0813f68a",
            "f4ca4948210049adaa80d2280129fcf2",
            "342e28bd5ed44f13822e00e83372bbd2",
            "027d80ef9f9941df85c4dfed63802467",
            "8ca5b53e43e147658ac5e0fd1ce3acaa",
            "61a1d7f93d4648119ef07290d8df6a1f",
            "c6d665af736143b3ad0de67b17e1d4cf",
            "c855ffae1c9b440da8372883bf0a60fe",
            "5b51be479d3e4ccfa08b637c565c5b58",
            "a8cfed2583364140a9fbe362ded5edf5",
            "20fc149d8d9c4d219a35199020980006",
            "0850b3efc2b044b79e7fb44095feb128",
            "0c51cddf793e4bf5b524207c1358bf2b",
            "f6ca2708fece4ccf94de28f5977c396e",
            "bfb7912523544f98855fbaa60ba40618",
            "2b4a5420109c48958e404e99a2acdee0",
            "83046b69de7a4c7e8d216864e2c9d0ef",
            "97353e235d164b64b97cf8e396d41e8c",
            "08b667d61d144f26a933f6696ee7a19f",
            "9788d6722ef44654bdd073e468d48da0",
            "6a60ada9b8d8474f9a7afc61675f092e",
            "5ea06da0e17642ea83c1a198ea02d6af",
            "07628b494b564548807bb40c3987c22e",
            "66020e4a546f4a0382de5edd00b462e0",
            "86a33c21ba5c491bb081fbd82f8e9e60",
            "d7bd277213c741a786805a0b0381d363",
            "648276672918482293a5bfa0871c4f14",
            "23d4f15865e14771a8145216323f7fa4",
            "54b8ecea12c34d25a54b338568bbcff6",
            "7e16af47cb1d47169e71f406b936a4a1",
            "a2f79d1c634a4f5288f533e823af53d3",
            "4ba3f0646ee849ff883e6110d3e385af",
            "7d59fa92faa64e9db1e8c0a2b5461aeb",
            "c6f9a25e022d489e8cd5f8663eafa866",
            "d2782967f50b4a2ba15ad8fedf4d4e28",
            "c9fa6167b51f4191b475f1ad1ff7c852",
            "58a15e9ff598470a8ac9cec8e33e57d6",
            "e31150edfdd148298247e2cc8d351acd",
            "927b3d0050b54c079206a5f64242b866",
            "936b2844b21040ada0ffc413c914da53",
            "c0964ea1330940c99ee6480b5f6586d1",
            "ffde8855995d4d01aa840185cb11d48a",
            "35bac7bd8eb64413854ec9e8d295e0a6",
            "2d694e75d2af445ea259cf68af93302c",
            "17718dc8db50425c9fa484c16ddc74ab",
            "9bb9b4be7fa249959d68e8e8c4b44265",
            "6ddd5dd5d59b4349851bc13ecdce02c9",
            "b49b11f980ac47ea9c47f7122665ce5a",
            "7f75a399e612431880713b6298bab9eb",
            "1ed8cf91b22f429dbeca989c4cae39a7",
            "1ebb42515f6944938abfdcd9922c7e5e",
            "716a6c9cae434b078fa2263c4e500386",
            "bc3b0a0596734202a86905a18bc0f649",
            "aa7a26e224ad447186a554438e9016aa",
            "935f2ee8d0804fc9904843444d116de9",
            "afc761e711c243ae8dd9a7ef85b8bc0b",
            "a97f07d6eb254d9e95d6cfdacebaa60b",
            "3a29e4e9d4244ec59f397b13317999c9",
            "c1479586d4454e22a373de39677e52bb",
            "27076d584e844712be735c5d965565ca",
            "979c397b85ed460d82584537c729325e",
            "70cf747d38864d199b088d43816b68ef",
            "cda12cf992104825bd359491eacea811",
            "088c9ddd4c984063ab8d9a47554aefc4",
            "85364c2d3f5c4c36a88d04b08f8981ed",
            "9843a3eb387f43feb0c1ec0ee5077a4a",
            "fe0f9b29e18540a4827249bd98462047",
            "726c701363974c718a7514643fc5ab41",
            "0013710e9ae94951b17e96cd420a8975",
            "5fb2b82b6e914feeabb33dbdd1fabaa3",
            "3fe410d002db430690f5e7922b2dc833",
            "a80711444fb949ffb57d76abbd28c83c",
            "64018d46551848d483e11afaf6846471",
            "2ce8feb20406476fac111f2f5fe8b49f",
            "2cf2f6f68cee4c3685892da559fa26c5",
            "e810b9724a4b40a58027f533d009e62f",
            "5d3f3a57c0b147ba94c5a1d9817f6d60",
            "aa60f19e7f5f4b35a4205ab9af11ef40",
            "fd27776aea5a4c8b81328084989ae1fa",
            "6e36842887a64d1fb72068236107e2f2",
            "9f1be440e248417ca45d8cd246b774bc",
            "b2c0a1a23907473f979f97fb4f975d97",
            "22a5e8bc4e8d4bc5b2e27eee3970f8ca",
            "c0f595a9016d43ddab955571aa62dcdb",
            "2f9c4620cd3349f5930fb6feae746f82",
            "9b86b4e5c2f846af8c9ce0d93753e13e",
            "5c9305241fb64da29861cc126a14708e",
            "c758f8d5be5649e1ac7be442cb7f03d3",
            "9e4e2dd055154019b9efec5e9885cbae",
            "c1e8903337164ea7beade4103f1267f9",
            "ed30b0dc17c14cdfb954253cdf3819f5",
            "3a4baf5377c94220a629ab95fc67f767",
            "b0ba270558d14080a8f8def6b2cd2684",
            "ea9ccb4df3b14e018cce787d60e9226d",
            "dff84e5d37e0422d86fa76ba20638a48",
            "d56a946bb58340b193884e4b02707569",
            "78e44b5105554fa9bc481d6cccf4466e",
            "9bb77c0bd03a4a48a194b737bc6a7966",
            "5dd9b0be8ad54d27b16dfe59c24c1909",
            "fd4ad77305d44eed81458c4bce001a4f"
          ]
        },
        "id": "TetSMf5JT-Iz",
        "outputId": "039fde6b-279e-451c-9248-a3ec0c1c5d82"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14a934778e8b4f5eb992657312585654",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efa4db0b40014cc0b7b9da3fa77134e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfbe05f155164ade847b667590706b70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4ca4948210049adaa80d2280129fcf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c51cddf793e4bf5b524207c1358bf2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66020e4a546f4a0382de5edd00b462e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2782967f50b4a2ba15ad8fedf4d4e28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bb9b4be7fa249959d68e8e8c4b44265",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a97f07d6eb254d9e95d6cfdacebaa60b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "726c701363974c718a7514643fc5ab41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd27776aea5a4c8b81328084989ae1fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1e8903337164ea7beade4103f1267f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZGERIeMT9-v"
      },
      "outputs": [],
      "source": [
        "def build_prompt(text):\n",
        "    return (\n",
        "        \"You are a professional summarization assistant. Your task is to read the following CV and generate a formal, concise summary in exactly three sentences. \"\n",
        "        \"The summary should synthesize key qualifications, areas of expertise, and notable achievements without repeating phrases or including any conversational elements. \"\n",
        "        \"Avoid lists, bullet points, or self-referential phrases. Focus on clarity, flow, and relevance for a technical audience:\\n\\n\"\n",
        "        f\"{text}\\n\\nSummary:\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzQVSk-yWJYb",
        "outputId": "108cff15-2e79-468f-af0b-2aeaec484f3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📄 mistralai/Mistral-7B-Instruct-v0.3 Summary:\n",
            "\n",
            "- Dimitris Dais is a Senior Machine Learning Engineer with a PhD in Artificial Intelligence and Civil Engineering.\n",
            "- He has extensive experience in ML, delivering end-to-end AI solutions for complex, real-world challenges.\n",
            "- He is skilled in defining problem scope, data strategy, and selecting optimal AI stacks.\n",
            "- Dimitris has a proven track record in AI innovation, with a strong hands-on experience in multimodal & generative AI, including Vision-Language Models, transformers, prompt engineering, zero/few-shot learning, and Retrieval-Augmented Generation (RAG) & Q&A.\n",
            "- He has a diverse background in academia and industry, with a focus on automating industrial inspections, building inspections, and earthquake engineering.\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=300\n",
        ")\n",
        "\n",
        "prompt = build_prompt(text)\n",
        "response = pipe(prompt)[0][\"generated_text\"]\n",
        "\n",
        "# Extract the summary part only\n",
        "summary = response.split(\"Summary:\")[-1].strip()\n",
        "print(f\"📄 {model_id} Summary:\\n\")\n",
        "for s in summary.split('. '):\n",
        "    s = s.strip()\n",
        "    if s:\n",
        "        print(f\"- {s.rstrip('.')}.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
