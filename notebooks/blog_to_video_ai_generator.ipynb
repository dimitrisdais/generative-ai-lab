{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa2d23f2",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dimitrisdais/generative-ai-lab/blob/main/notebooks/blog_to_video_ai_generator.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee2ec7db",
      "metadata": {
        "id": "ee2ec7db"
      },
      "source": [
        "# üé¨ AI Blog-to-Video Generator\n",
        "\n",
        "üëã Hi, I'm **Dimitris Dais**, an engineer passionate about AI creativity tools.\n",
        "\n",
        "This notebook turns a blog post into a complete narrated video using:\n",
        "\n",
        "Step 1: Generate the Blog Structure with an LLM  \n",
        "Step 2: Write Paragraphs for Each Section with an LLM  \n",
        "Step 3: Create Visuals for Each Section with a Text-to-Image Model  \n",
        "Step 4: Generate Audio Narration with a Text-to-Speech Model  \n",
        "Step 5: Combine Frames and Audio into the Final Video  \n",
        "\n",
        "For more explanation, refer to the [corresponding blog](https://dimitrisdais.github.io/dimitris-dais.github.io/ai/multimodal/blog_to_video_ai_generator/).\n",
        "\n",
        "Enjoyed it? Reuse or expand it ‚Äî and feel free to connect.\n",
        "\n",
        "üîó **Website**: [dimitrisdais.github.io](https://dimitrisdais.github.io/dimitris-dais.github.io/)  \n",
        "üì¨ **Contact**: dimitris.dais.phd@gmail.com  \n",
        "üêô **GitHub**: [@dimitrisdais](https://github.com/dimitrisdais)  \n",
        "üîó **LinkedIn**: [linkedin.com/in/dimitris-dais](https://www.linkedin.com/in/dimitris-dais/)  \n",
        "‚ñ∂Ô∏è **YouTube**: [youtube.com/@dimitrisdais](https://www.youtube.com/channel/UCuSdAarhISVQzV2GhxaErsg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62521581",
      "metadata": {
        "id": "62521581"
      },
      "source": [
        "### üîó Mount Google Drive\n",
        "\n",
        "This step connects your Google Drive to the Colab notebook so that we can save and access files like generated images, audio, and videos.\n",
        "\n",
        "‚û°Ô∏è A pop-up will appear asking you to **authorize** access ‚Äî click the link, choose your Google account, and copy the authorization code back into the notebook when prompted\n",
        "\n",
        "![Mount Google Drive](https://raw.githubusercontent.com/dimitrisdais/generative-ai-lab/main/assets/images/mount_google_drive.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "641e42da",
      "metadata": {
        "id": "641e42da"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9410d314",
      "metadata": {
        "id": "9410d314"
      },
      "source": [
        "### üîß Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fqD3AglWcAua",
      "metadata": {
        "id": "fqD3AglWcAua"
      },
      "outputs": [],
      "source": [
        "!pip install -U TTS transformers==4.46.2 accelerate==1.1.1 bitsandbytes==0.45.2 datasets==3.1.0 huggingface-hub==0.26.2 safetensors==0.4.5 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cdec91d",
      "metadata": {
        "collapsed": true,
        "id": "9cdec91d"
      },
      "outputs": [],
      "source": [
        "!apt-get update && apt-get install -y espeak libespeak1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5kCpGA3idV7h",
      "metadata": {
        "id": "5kCpGA3idV7h"
      },
      "outputs": [],
      "source": [
        "import ctypes; ctypes.cdll.LoadLibrary(\"libespeak.so.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VgIS5D6NR1Rw",
      "metadata": {
        "id": "VgIS5D6NR1Rw"
      },
      "source": [
        "### ‚öôÔ∏è Check for GPU Availability\n",
        "\n",
        "This step checks whether a GPU is available in the current Colab environment and assigns the appropriate device (`\"cuda\"` for GPU or `\"cpu\"` otherwise).  \n",
        "Using a GPU can significantly speed up model inference for image and audio generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TldQjV5JQCJB",
      "metadata": {
        "id": "TldQjV5JQCJB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c92c586c",
      "metadata": {
        "id": "c92c586c"
      },
      "source": [
        "### üìÅ Create Output Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d08bec90",
      "metadata": {
        "id": "d08bec90"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "base_dir = \"/content/blog_ai_video\"\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "session_dir = os.path.join(base_dir, timestamp)\n",
        "audio_dir = os.path.join(session_dir, \"audio\")\n",
        "frames_dir = os.path.join(session_dir, \"frames\")\n",
        "video_dir = os.path.join(session_dir, \"video\")\n",
        "\n",
        "for d in [audio_dir, frames_dir, video_dir]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"Output directory created:\", session_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd5127a",
      "metadata": {
        "id": "3fd5127a"
      },
      "source": [
        "## üí° Step 1: Generate the Blog Structure with an LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DD1PwD5VTzmV",
      "metadata": {
        "id": "DD1PwD5VTzmV"
      },
      "source": [
        "### üîê Authenticate with Hugging Face Hub\n",
        "\n",
        "To access certain models (especially large or gated ones), you may need to log in to your Hugging Face account.\n",
        "\n",
        "This step will prompt you to enter your **Hugging Face access token**, which you can obtain from [your account's tokens page](https://huggingface.co/settings/tokens).  \n",
        "Once authenticated, the notebook will have permission to download and use models that require login.\n",
        "\n",
        "![Hugging Face - Login](https://raw.githubusercontent.com/dimitrisdais/generative-ai-lab/main/assets/images/hugging_face_login.png)  \n",
        "\n",
        "![Hugging Face - Access Repository](https://raw.githubusercontent.com/dimitrisdais/generative-ai-lab/main/assets/images/hugging_face_access_repository.png)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m6CEG_6xTY5y",
      "metadata": {
        "id": "m6CEG_6xTY5y"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bz2cboeKODkr",
      "metadata": {
        "id": "Bz2cboeKODkr"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14aad115",
      "metadata": {
        "id": "14aad115"
      },
      "outputs": [],
      "source": [
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "indlR5WaN0P2",
      "metadata": {
        "id": "indlR5WaN0P2"
      },
      "outputs": [],
      "source": [
        "# Generate blog title\n",
        "title_prompt = \"Suggest one short and funny blog post title. Avoid using listicles or numbers like '10 ways'.\"\n",
        "\n",
        "# Generate with sampling\n",
        "output = generator(\n",
        "    title_prompt,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "# Extract clean title\n",
        "blog_title = output[0][\"generated_text\"].replace(title_prompt, \"\").strip()\n",
        "del output\n",
        "\n",
        "# Print result\n",
        "print(\"üìò Blog Title:\\n\", blog_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z0D3M7nUN1lQ",
      "metadata": {
        "id": "z0D3M7nUN1lQ"
      },
      "outputs": [],
      "source": [
        "# Generate section titles\n",
        "section_prompt = f\"### Instruction:\\nWrite five short and creative blog section titles for a blog titled: '{blog_title}'. Format them as a numbered list.\\n### Response:\"\n",
        "\n",
        "section_output = generator(\n",
        "    section_prompt,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.9\n",
        ")[0][\"generated_text\"]\n",
        "\n",
        "section_titles = re.findall(r\"\\d+\\.\\s*(.*?)(?=\\n\\d+\\.|$)\", section_output.strip())\n",
        "print(\"üìë Sections:\\n\")\n",
        "for t in section_titles: print(\"-\", t)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab02935",
      "metadata": {
        "id": "eab02935"
      },
      "source": [
        "## ‚úçÔ∏è Step 2: Write Paragraphs for Each Section with an LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f22c3d",
      "metadata": {
        "id": "e8f22c3d"
      },
      "outputs": [],
      "source": [
        "blog = {}\n",
        "\n",
        "for title in section_titles:\n",
        "\n",
        "    prompt = f\"### Instruction:\\nWrite a short paragraph (2-3 sentences) for a blog section titled: '{title}'.\\n### Response:\"\n",
        "\n",
        "    response = generator(prompt, max_new_tokens=200, do_sample=True, temperature=0.9)[0][\"generated_text\"]\n",
        "    blog[title] = response.replace(prompt, \"\").strip()\n",
        "    print(f\"üìù {title}:\\n\", blog[title], \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d055ce9c",
      "metadata": {
        "id": "d055ce9c"
      },
      "source": [
        "## üé® Step 3: Create Visuals for Each Section with a Text-to-Image Model\n",
        "\n",
        "This step uses a Stable Diffusion model to generate images for each blog section. For every section title, the model creates a relevant visual based on the provided text prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-v-3G14wOGHQ",
      "metadata": {
        "id": "-v-3G14wOGHQ"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f735f6c8",
      "metadata": {
        "id": "f735f6c8"
      },
      "outputs": [],
      "source": [
        "sd_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    torch_dtype=torch.float32\n",
        ")\n",
        "sd_pipe = sd_pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uO3aVHWoOJyM",
      "metadata": {
        "id": "uO3aVHWoOJyM"
      },
      "outputs": [],
      "source": [
        "image_paths = []\n",
        "\n",
        "for i, title in enumerate(blog.keys()):\n",
        "\n",
        "    prompt = f\"An eye-catching illustration representing: {title}\"\n",
        "\n",
        "    print(f\"Generating image {i+1} for: '{title}'\")\n",
        "\n",
        "    image = sd_pipe(prompt).images[0]\n",
        "    path = f\"{frames_dir}/frame_{i:02d}.png\"\n",
        "    image.save(path)\n",
        "    image_paths.append(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HTdCYwSUOLIf",
      "metadata": {
        "id": "HTdCYwSUOLIf"
      },
      "outputs": [],
      "source": [
        "# Show images\n",
        "from IPython.display import display\n",
        "for p in image_paths:\n",
        "    display(Image.open(p))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6439f29",
      "metadata": {
        "id": "d6439f29"
      },
      "source": [
        "## üîä Step 4: Generate Audio Narration with a Text-to-Speech Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ENNYDUgmONrr",
      "metadata": {
        "id": "ENNYDUgmONrr"
      },
      "outputs": [],
      "source": [
        "from TTS.api import TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90fa0e8d",
      "metadata": {
        "id": "90fa0e8d"
      },
      "outputs": [],
      "source": [
        "tts = TTS(\n",
        "    model_name=\"tts_models/en/vctk/vits\",\n",
        "    progress_bar=False,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IDbaiIs9OQDL",
      "metadata": {
        "id": "IDbaiIs9OQDL"
      },
      "outputs": [],
      "source": [
        "audio_paths = []\n",
        "\n",
        "for i, (title, text) in enumerate(blog.items()):\n",
        "    audio_path = os.path.join(audio_dir, f\"section_{i:02d}.wav\")\n",
        "    tts.tts_to_file(text=text, file_path=audio_path, speaker=\"p225\")\n",
        "    audio_paths.append(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ltbMBJK4ORlw",
      "metadata": {
        "id": "ltbMBJK4ORlw"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "for path in audio_paths:\n",
        "    display(Audio(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2108b794",
      "metadata": {
        "id": "2108b794"
      },
      "source": [
        "## üé¨ Step 5: Combine Frames and Audio into the Final Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mx9cL2qsfqQr",
      "metadata": {
        "id": "mx9cL2qsfqQr"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "for i in range(5):  # Adjust loop if needed\n",
        "    frame_path = os.path.join(frames_dir, f\"frame_{i:02d}.png\")\n",
        "    audio_path = os.path.join(audio_dir, f\"section_{i:02d}.wav\")\n",
        "    out_video_path = os.path.join(video_dir, f\"clip_{i:02d}.mp4\")\n",
        "\n",
        "    print(f\"üîß Generating video {i + 1}/5\")\n",
        "    print(f\"üì§ Output path: {out_video_path}\")\n",
        "\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-y\",\n",
        "        \"-loop\", \"1\", \"-i\", frame_path,\n",
        "        \"-i\", audio_path,\n",
        "        \"-c:v\", \"libx264\", \"-tune\", \"stillimage\",\n",
        "        \"-shortest\", \"-vf\", \"scale=1280:720\",\n",
        "        \"-pix_fmt\", \"yuv420p\",\n",
        "        out_video_path\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        subprocess.run(cmd, check=True)\n",
        "        print(\"‚úÖ Video created successfully.\\n\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Failed to generate video {i}: {e}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "okXrF9u7fQFv",
      "metadata": {
        "id": "okXrF9u7fQFv"
      },
      "outputs": [],
      "source": [
        "# Create a text file that lists all video parts\n",
        "with open(os.path.join(video_dir, \"file_list.txt\"), \"w\") as f:\n",
        "    for i in range(5):\n",
        "        f.write(f\"file 'clip_{i:02d}.mp4'\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nx7NMTIdgqLw",
      "metadata": {
        "id": "nx7NMTIdgqLw"
      },
      "outputs": [],
      "source": [
        "# Paths to your input list file and output video\n",
        "video_list_path = os.path.join(video_dir, \"file_list.txt\")\n",
        "video_output_path = os.path.join(session_dir, \"final_blog_video.mp4\")\n",
        "\n",
        "print(f\"üìã Concatenating videos listed in: {video_list_path}\")\n",
        "print(f\"üì§ Final output path: {video_output_path}\")\n",
        "\n",
        "# FFmpeg concat command using subprocess to avoid UTF-8 locale error in Colab\n",
        "cmd = [\n",
        "    \"ffmpeg\", \"-y\",\n",
        "    \"-f\", \"concat\", \"-safe\", \"0\",\n",
        "    \"-i\", video_list_path,\n",
        "    \"-c\", \"copy\",\n",
        "    video_output_path\n",
        "]\n",
        "\n",
        "try:\n",
        "    subprocess.run(cmd, check=True)\n",
        "    print(\"‚úÖ Final video assembled successfully!\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"‚ùå FFmpeg failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "619d5fba",
      "metadata": {
        "id": "619d5fba"
      },
      "source": [
        "### Copy Output to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e3d07b2",
      "metadata": {
        "id": "9e3d07b2"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "drive_path = \"/content/drive/MyDrive/blog_ai_video_output\"\n",
        "dest_path = os.path.join(drive_path, timestamp)\n",
        "\n",
        "shutil.copytree(session_dir, dest_path, dirs_exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Copied everything to Google Drive at: {dest_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
